---
title: "[LG Aimers] Reinforcement Learning II"
description: "강화학습 기초: Monte Carlo와 Temporal Difference"
date: 2025-07-21
published: true
tags: ["LG Aimers", "Reinforcement Learning", "AI"]
---

> 본 포스팅은 LG Aimers 7기 Phase 1 강화학습 강의 내용을 정리한 글입니다. 모든 출처는 LG Aimers에 있습니다.
---

## **Part III – 모델이 없는 상황에서의 Value 평가**

### 1. **실제 환경과 MDP의 차이**

* 현실에서 우리는 환경의 모든 정보를 알 수 없음.
* 전이 확률 $P(s'|s,a)$나 보상 함수 $R(s,a)$가 주어지지 않고, **단편적인 샘플**만 관측할 수 있음.
* 이러한 상황에서 Value Function을 어떻게 평가할 수 있을까?

---

## **Part IV – Monte Carlo 방법**

### 1. **Monte Carlo Policy Evaluation**

* 전이 확률이나 보상 모델이 **전혀 필요 없음**.
* **Markov 가정도 불필요** → MDP 외의 상황에서도 사용 가능.
* 단점: 반드시 **에피소드가 종료**되어야 리턴 계산이 가능함 (episodic 필수).

---

### 2. **First-Visit Monte Carlo**

* 어떤 상태 $s$에 대해 **처음 방문한 시점 t**에만 Return $G_t$를 기록.
* 이 값을 누적하여 평균을 계산 → $V(s) = \frac{1}{N(s)} \sum_{i=1}^{N(s)} G^{(i)}$

* **Unbiased Estimator**: 기대값이 실제 $V^\pi(s)$와 같음.
* **Consistent Estimator**: 샘플 수가 증가하면 추정값이 실제 값에 수렴.

---

### 3. **Every-Visit Monte Carlo**

* 에피소드 내에서 동일한 상태를 **여러 번 방문**하더라도 **모든 방문 지점**의 Return을 사용함.
* **Variance가 낮지만**, **Bias가 존재**하는 Estimator.
* 평균 제곱 오차(MSE) 관점에서 보면 일반적으로 First-Visit보다 우수.

---

### 4. **Monte Carlo의 점진적(Incremental) 형태**

* 수식을 재귀적으로 표현하면 다음과 같음:

  $$
  V(s) \leftarrow V(s) + \alpha \left( G_t - V(s) \right)
  $$

* 학습률 $\alpha$는 일반적으로 $\frac{1}{N(s)}$ 또는 하이퍼파라미터로 고정.

---

## **Part V – Temporal Difference (TD) 학습**

### 1. **TD Learning의 동기**

* MC는 **에피소드가 끝날 때까지** 기다려야 함 → 실시간 학습 불가능.
* TD는 **한 번의 transition만 보고도** 학습 가능.
* **Dynamic Programming의 부트스트랩 개념**과 Monte Carlo의 샘플 기반 특성을 결합한 방식.

---

### 2. **TD(0) Policy Evaluation**

* 다음 상태 $s_{t+1}$에서 관측한 리워드와 현재 Value를 이용:

  $$
  V(s_t) \leftarrow V(s_t) + \alpha \left( r_{t} + \gamma V(s_{t+1}) - V(s_t) \right)
  $$

* $r_t + \gamma V(s_{t+1})$는 Bellman 기대값의 샘플 버전 (TD target).
* 에피소드가 끝나지 않아도 즉시 업데이트 가능.

---

### 3. **TD vs MC vs DP**

| 항목 | Monte Carlo (MC) | Temporal Difference (TD) | Dynamic Programming (DP) |
|------|------------------|---------------------------|---------------------------|
| 모델 필요 여부 | 불필요 | 불필요 | 필요 |
| 샘플 기반 학습 | O | O | X |
| 부트스트랩 사용 | X | O | O |
| 에피소드 필요 | O | X | X |
| 수렴 속도 | 느림 (높은 분산) | 빠름 (낮은 분산) | 빠름 |
| 오차 안정성 | 안정적 | 초기값 민감 | 안정적 |

---

### 4. **Certainty Equivalent Learning**

* TD와 MC는 **샘플 기반 추정**만 사용하지만,
* **모든 샘플을 수집하여 최대우도추정(MLE)**을 통해 $P$, $R$을 추정하고 **DP 방식으로 해결**.
* **모델 기반 접근법**이며, 계산량은 크지만 **데이터 효율성 가장 높음**.
* 단점: 매 스텝마다 파라미터 재추정 → 비효율적일 수 있음.

---

### 5. **요약: MC, TD, DP, CE의 차이점**

| 방법 | 모델 필요 | 부트스트랩 | 샘플 사용 | 에피소드 필요 | 장점 | 단점 |
|------|-----------|------------|------------|----------------|------|------|
| MC | X | X | O | O | 안정적 추정, 단순 | 높은 분산, 느림 |
| TD | X | O | O | X | 빠른 수렴, 온라인 학습 | 초기값 민감 |
| DP | O | O | X | X | 계산 정확도 높음 | 모델 필요 |
| CE | O (추정) | O | O | X | 데이터 효율성 | 계산 복잡도 큼 |

---

## **정리**

* 환경의 모델을 모를 때, MC나 TD는 유용한 **모델 프리 학습 방법**이다.
* MC는 전체 에피소드를 통해 value를 추정하고, TD는 한 스텝의 transition만으로 학습한다.
* 둘의 학습 과정은 다르지만, 충분한 반복을 통해 **동일한 최종 값에 수렴**할 수 있다.
* 선택은 **데이터 조건, 요구되는 반응 시간, 안정성** 등에 따라 결정된다.

---
