---
title: "[Co-Week Academy] 코드 거대 언어 모델 (LLM for Code Intelligence)"
description: "코드를 위한 거대 언어 모델의 구조, 학습 방식, 응용 및 도전 과제 정리"
date: 2025-07-10
published: true
tags: ["Co-Week Academy", "LLM", "Code Intelligence", "AI", "Deep Learning", "Transformer"]
---

> 본 포스팅은 2025 Co-Week Academy 참여 후 강의 내용을 정리한 글입니다.

# 코드 거대 언어 모델 (LLM for Code Intelligence)

## 서론: 코드란 무엇인가?

- **소스코드도 언어**다. 하지만 자연어와는 본질적으로 다르다.
- 자연어는 문법이 조금 틀려도 의미가 전달된다. 코드는 **한 글자만 틀려도 실행되지 않는다.**
- 코드의 구성 요소: 함수명, 파라미터, 키워드, 식별자, 제어/데이터 흐름 등
- 코드도 텍스트이지만 **더 포멀하고 엄격**한 규칙을 따른다.

---

## 코드 표현 방식: 시퀀스 vs 그래프

### 시퀀스 기반
- 코드 = 토큰의 나열
- Transformer, RNN 등 사용
- GPT, CodeGPT 등은 이 구조 사용
- 장점: 기존 NLP 기술 활용 쉬움  
- 단점: 구조적 정보 반영 어려움

### 그래프 기반
- 코드 = 구조의 집합
- AST(Abstract Syntax Tree), CFG, DFG, PDG 등
- Graph Neural Network, GraphCodeBERT
- 장점: **문법, 데이터 흐름, 제어 흐름**까지 모델링 가능  
- 단점: 구조 추출 비용, 복잡성 증가

---

## 코드 인텔리전스 태스크

### 1. 이해 중심 태스크
- **Code Search**: 자연어 질의 → 관련 코드 찾기
- **Code Classification**: 언어, 기능, 오류 여부 분류
- **Bug/Vulnerability Detection**
- **Code Clone Detection**
- **Code Embedding / Representation**

### 2. 생성 중심 태스크
- **Code Summarization**: 코드 → 자연어 주석
- **Code Generation**: 요구사항 → 코드 생성
- **Code Translation**: 언어 간 변환 (ex. Java → Python)
- **Error Repair**, **Test-case Generation**
- **Code QA**, **Repo-level Code Synthesis**

---

## 사전학습 모델 구조

### 인코더 기반 (이해 중심)
- CodeBERT
- GraphCodeBERT
- CodeT5 (Encoder part)

### 디코더 기반 (생성 중심)
- GPT, CodeGen, Code LLaMA
- DeepSeek Coder
- CodeGemma

### 인코더+디코더 기반 (양방향)
- PLBART
- CodeT5, CodeT5+
- UniXcoder

---

## 학습 데이터셋

### CodeSearchNet (6개 언어, 6GB)
- 코드 + 커멘트 페어 데이터
- 주로 고품질 위주, 데이터 규모는 작음

### The Stack (BigCode: 700+ 언어, 4.5TB)
- 대규모 코드 수집 데이터셋
- GitHub 기반, 노이즈 있음
- 최근 LLM은 대부분 이 기반

---

## 대표 코드 LLM 모델

| 모델 | 구조 | 특징 |
|------|------|------|
| **CodeBERT** | 인코더 | PL & NL 이중모달 학습 |
| **GraphCodeBERT** | 인코더 | AST + Data Flow 활용 |
| **PLBART** | 인코+디코 | Summarization, Translation, Generation 모두 가능 |
| **CodeT5 / CodeT5+** | 인코+디코 | Identifier-aware 학습, 대규모 버전 확장 |
| **UniXcoder** | 통합형 | 인코더/디코더 모드 전환 |
| **CodeLlama** | 디코더 | LLaMA 기반 대형 생성모델 |
| **DeepSeek-Coder** | 디코더 | 338개 언어 지원, 128K 토큰 |
| **CodeGemma** | 디코더 | Gemini 구조, 빠른 인퍼런스 |
| **StarCoder 2** | 디코더 | 최신 공개형 코드 특화 모델 |

---

## 기술적 도전 과제

### 1. 자연어와 코드 간 의미 격차 (Semantic Gap)
- Text-to-SQL 등 멀티모달 입력 → 코드 출력 문제
- **Schema Linking**, **Retrieval-Augmented Generation(RAG)** 필요

### 2. Robustness
- Dead Code 삽입, Identifier 교체 등에 성능 저하
- 단순 Lexical Matching이 아닌 **구문/의미적 이해** 필요

### 3. Hallucination (환각)
- 존재하지 않는 API 호출, 문법 오류 코드 생성
- **컴파일러 기반 검증**, **에이전트 기반 재생성 루프** 활용 시도

### 4. 평가의 어려움
- 기존 BLEU, CodeBLEU → 실행 기반 평가 필요 (e.g. HumanEval)
- "코드가 문법적으로 맞는가"보다 "주어진 입력 → 올바른 출력 생성"이 더 중요

### 5. 멀티모달 코드 처리
- 테이블, 이미지, 로그 등 다양한 데이터와 코드의 통합
- 예: Text + Table → SQL, Vision + Command → Script

### 6. 에이전트 기반 코드 생성
- 단일 모델이 아닌, 복수 에이전트가 **계획–검증–보정–완성** 단계를 나눠 수행
- GPT도 내부적으로 실행 후 결과 기반 최종 출력 구성한다고 추정됨

---